Production-grade Spark requirements (YOU solve these)

Now the fun part.
These are concrete, solvable Spark Scala tasks using your datasets.

â¸»

ğŸ”¥ Requirement 1: Transaction Deduplication

Problem
â€¢	Same transaction_id may appear multiple times
â€¢	Keep the latest ingest_time per transaction

Constraints
â€¢	Must work with skew
â€¢	Must not full-scan history every day

ğŸ‘‰ Forces:
â€¢	Window functions
â€¢	Incremental logic
â€¢	Partition pruning

â¸»

ğŸ”¥ Requirement 2: Daily Customer Spend (Correct with Late Data)

Problem
â€¢	Compute daily spend per customer
â€¢	Late transactions can arrive 3 days late
â€¢	Must update historical aggregates

ğŸ‘‰ Forces:
â€¢	Reprocessing windows
â€¢	Idempotent jobs
â€¢	Event-time thinking

â¸»

ğŸ”¥ Requirement 3: SCD2 Customer Join

Problem
â€¢	Join transactions to customer attributes
â€¢	Correct attributes as of transaction time

ğŸ‘‰ Forces:
â€¢	Range joins
â€¢	Window logic
â€¢	Join optimization

â¸»

ğŸ”¥ Requirement 4: Fraud-like Velocity Signal

Problem
â€¢	Flag accounts with:
â€¢	5 txns in 2 minutes
â€¢	amount spike vs rolling avg

ğŸ‘‰ Forces:
â€¢	Window aggregations
â€¢	State management (batch style first)

â¸»

ğŸ”¥ Requirement 5: Data Quality Pipeline

Problem
â€¢	Separate:
â€¢	valid transactions
â€¢	quarantined transactions
â€¢	Emit metrics

ğŸ‘‰ Forces:
â€¢	Schema enforcement
â€¢	Conditional routing
â€¢	Accumulators / metrics



1ï¸âƒ£ Convert your DataGenerator into a Spark-based scalable generator
2ï¸âƒ£ Give exact Scala Spark skeletons for each requirement (no solution logic)
3ï¸âƒ£ Design performance experiments (AQE on/off, salting, broadcast)
4ï¸âƒ£ Turn this into an interview-grade Spark project

